# veScale

## Coming Soon

We are refactoring our internal LLM training sytem components to meet open source standard. The tentative timeline is as follows:

1. by mid April, 4D parallelism (tensor parallelism, sequence parallelism, data parallelism and ZERO) examples for nanoGPT, Llama2 and Mixtral models
2. by end of May, fast checkpointing system
3. by end of July, CUDA event monitor, pipeline parallelism and supporting components for large-scale training

## [License](./LICENSE)

The veScale Project is under the Apache License v2.0.
